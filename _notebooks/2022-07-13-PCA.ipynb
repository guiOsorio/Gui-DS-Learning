{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f31def8d",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "> Principal Component Analysis\n",
    "\n",
    "- toc: true\n",
    "- branch: true\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Gui Osorio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984d843",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA for short), is a technique used to reduce the dimensionality of data. Let's say we have a dataset of dimension 100x250 (100 rows, 250 columns), and we want to use that to train a machine learning model. Using all of this dataset's features is impractical and inneficient.\n",
    "\n",
    "To overcome this, we can use PCA, which will reduce the dimensionality of our data and facilitate building our ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca7a49",
   "metadata": {},
   "source": [
    "## PCA from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb1173c",
   "metadata": {},
   "source": [
    "Let's go through the Principal Component Analysis algorithm step-by-step using only NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "666efc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b774105",
   "metadata": {},
   "source": [
    "First, let's define a 3x2 matrix M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5191fbe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4],\n",
       "       [5, 6]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M = np.array([[1,2], [3,4], [5,6]])\n",
    "M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9d999f",
   "metadata": {},
   "source": [
    "Our first step is to calculate the mean values across each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d251163",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_means = np.mean(M, axis=0)\n",
    "# in the axis parameter, we can specify in which direction to calculate the means: 0 for columns, 1 for rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11994e66",
   "metadata": {},
   "source": [
    "Next, we need to center the column values by subtracting the respective mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56baca5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -2.],\n",
       "       [ 0.,  0.],\n",
       "       [ 2.,  2.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centered_M = M - col_means\n",
    "centered_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109a87c",
   "metadata": {},
   "source": [
    "Once we have the centered matrix, we can calculate the covariance matrix of its transpose.\n",
    "\n",
    "A covariance matrix gives us the correlation scores between columns in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ee857fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 4.],\n",
       "       [4., 4.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covar = np.cov(centered_M.T)\n",
    "covar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c51b384",
   "metadata": {},
   "source": [
    "Next, we need to calculate the eigendecomposition of this covariance matrix, and store the resulting eigenvectors and eigenvalues.\n",
    "\n",
    "Eigenvectors represent the directions for the reduced matrix, whereas the eigenvalues represent the magnitudes of these directions.\n",
    "\n",
    "Eigenvalues close to 0 represent components which are not relevant. When performing PCA, we need to choose the top k eigenvalues to keep, which will represent the most relevant components of the features (called principals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66b69f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8. 0.]\n",
      "--------------------------------\n",
      "[[ 0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(covar)\n",
    "print(eig_vals) # eigenvalues close to 0 represent components which are not relevant\n",
    "print('--------------------------------')\n",
    "print(eig_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13524218",
   "metadata": {},
   "source": [
    "Finally, it's time to project our transformed data. This will correspond to dt product of the transpose of our eigenvectors and the transpose of the centered matrix calculated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc84dc89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.82842712,  0.        ],\n",
       "       [ 0.        ,  0.        ],\n",
       "       [ 2.82842712,  0.        ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_M = eig_vecs.T.dot(centered_M.T).T\n",
    "transformed_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3a8399",
   "metadata": {},
   "source": [
    "This result indicates that we can transform our original 3x2 matrix into a 3x1 matrix with minimal loss (this could have already been concluded after noticing an eigenvalue of 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d6568",
   "metadata": {},
   "source": [
    "## PCA with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e623a",
   "metadata": {},
   "source": [
    "Now, let's make things easy and use scikit-learn to help in the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09907faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a956f5",
   "metadata": {},
   "source": [
    "We can easily achieve the same results with fewer lines of code using scikit-learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473be403",
   "metadata": {},
   "source": [
    "First, let's initialize a PCA class and fit it into our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51355bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>PCA(n_components=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PCA</label><div class=\"sk-toggleable__content\"><pre>PCA(n_components=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "PCA(n_components=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize\n",
    "pca = PCA(2)\n",
    "\n",
    "# Fit\n",
    "pca.fit(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d59ac",
   "metadata": {},
   "source": [
    "After fitting the model, we have access to all of the values calculated in the NumPy approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce808ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EIGENVALUES\n",
      "[8.00000000e+00 2.25080839e-33]\n",
      "--------------------------------\n",
      "EIGENVECTORS\n",
      "[[ 0.70710678  0.70710678]\n",
      " [ 0.70710678 -0.70710678]]\n",
      "--------------------------------\n",
      "TRANSFORMED MATRIX\n",
      "[[-2.82842712e+00  2.22044605e-16]\n",
      " [ 0.00000000e+00  0.00000000e+00]\n",
      " [ 2.82842712e+00 -2.22044605e-16]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalues\n",
    "print('EIGENVALUES')\n",
    "print(pca.explained_variance_)\n",
    "print('--------------------------------')\n",
    "\n",
    "# Eigenvectors\n",
    "print('EIGENVECTORS')\n",
    "print(pca.components_)\n",
    "print('--------------------------------')\n",
    "\n",
    "# Transformed data\n",
    "transformed_M2 = pca.transform(M)\n",
    "print('TRANSFORMED MATRIX')\n",
    "print(transformed_M2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0810553",
   "metadata": {},
   "source": [
    "**You have now been introduced to a valuable dimensionality reduction technique, the Principal Component Analysis!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
